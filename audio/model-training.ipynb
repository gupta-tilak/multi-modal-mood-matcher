{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cdbc71",
   "metadata": {},
   "source": [
    "Multi-Modal Mood Matcher - Audio Feature Extraction Training Notebook\n",
    "======================================================================\n",
    "This notebook trains audio emotion recognition models using:\n",
    "1. Wav2Vec2/HuBERT for self-supervised speech features\n",
    "2. CNN on mel-spectrograms with temporal modeling\n",
    "3. ECAPA-TDNN for speaker-aware emotion features\n",
    "\n",
    "Target: Extract emotion-aware temporal embeddings from speech segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6425362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SpeechBrain not available: AttributeError\n",
      "   This is optional - Wav2Vec2 and CNN models will still work fine.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Audio Processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Transformers for Wav2Vec2/HuBERT\n",
    "from transformers import (\n",
    "    Wav2Vec2Model, \n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2Config,\n",
    "    HubertModel,\n",
    "    AutoProcessor,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# SpeechBrain for ECAPA-TDNN (optional)\n",
    "try:\n",
    "    from speechbrain.pretrained import EncoderClassifier\n",
    "    SPEECHBRAIN_AVAILABLE = True\n",
    "    print(\"✓ SpeechBrain available\")\n",
    "except (ImportError, AttributeError) as e:\n",
    "    SPEECHBRAIN_AVAILABLE = False\n",
    "    print(f\"⚠️ SpeechBrain not available: {type(e).__name__}\")\n",
    "    print(\"   This is optional - Wav2Vec2 and CNN models will still work fine.\")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41308afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724b7c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddc20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = \"./data/ravdess\"  # RAVDESS dataset directory\n",
    "    OUTPUT_DIR = \"./outputs/audio_emotion_model\"\n",
    "    CHECKPOINT_DIR = \"./checkpoints/audio\"\n",
    "    \n",
    "    # Model Architecture\n",
    "    MODEL_TYPE = \"wav2vec2\"  # Options: wav2vec2, hubert, cnn_spectrogram\n",
    "    PRETRAINED_MODEL = \"facebook/wav2vec2-base\"  # or \"facebook/hubert-base-ls960\"\n",
    "    \n",
    "    # Audio Parameters\n",
    "    SAMPLE_RATE = 16000  # Standard for speech models\n",
    "    MAX_DURATION = 10.0  # seconds\n",
    "    MIN_DURATION = 1.0\n",
    "    \n",
    "    # Feature Extraction (for CNN-based models)\n",
    "    N_MELS = 128\n",
    "    N_MFCC = 40\n",
    "    HOP_LENGTH = 512\n",
    "    N_FFT = 2048\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    EMBEDDING_DIM = 768  # Wav2Vec2: 768, HuBERT: 768\n",
    "    HIDDEN_DIM = 512\n",
    "    NUM_CLASSES = 4  # RAVDESS 4-class: neutral, happy, sad, angry\n",
    "    USE_LSTM = True  # Add LSTM for temporal modeling\n",
    "    LSTM_LAYERS = 2\n",
    "    LSTM_BIDIRECTIONAL = True\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    BATCH_SIZE = 16  # Audio models are memory intensive\n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 5e-5  # Lower LR for fine-tuning pretrained models\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    WARMUP_STEPS = 500\n",
    "    SCHEDULER = \"linear_warmup\"\n",
    "    \n",
    "    # Training Strategy\n",
    "    FREEZE_ENCODER_EPOCHS = 3  # Freeze Wav2Vec2 encoder initially\n",
    "    GRAD_CLIP = 1.0\n",
    "    MIXED_PRECISION = True\n",
    "    ACCUMULATION_STEPS = 2  # Gradient accumulation for larger effective batch size\n",
    "    \n",
    "    # Data Augmentation\n",
    "    USE_AUGMENTATION = True\n",
    "    AUGMENT_PROB = 0.5\n",
    "    \n",
    "    # Early Stopping\n",
    "    PATIENCE = 8\n",
    "    MIN_DELTA = 0.001\n",
    "    \n",
    "    # Logging\n",
    "    USE_WANDB = False\n",
    "    WANDB_PROJECT = \"audio-emotion-embedding\"\n",
    "    \n",
    "    # RAVDESS Emotion Labels (4-class mapping)\n",
    "    # RAVDESS codes: 01=neutral, 03=happy, 04=sad, 05=angry\n",
    "    EMOTION_LABELS = {\n",
    "        0: 'neutral',\n",
    "        1: 'happy',\n",
    "        2: 'sad',\n",
    "        3: 'angry'\n",
    "    }\n",
    "    \n",
    "    # RAVDESS to our label mapping\n",
    "    RAVDESS_EMOTION_MAP = {\n",
    "        '01': 0,  # neutral\n",
    "        '03': 1,  # happy\n",
    "        '04': 2,  # sad\n",
    "        '05': 3,  # angry\n",
    "        # Excluded: '02': calm, '06': fearful, '07': disgust, '08': surprised\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e4c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. AUDIO PREPROCESSING AND AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"Audio preprocessing utilities\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000, max_duration=10.0):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_duration = max_duration\n",
    "        self.max_samples = int(sample_rate * max_duration)\n",
    "    \n",
    "    def load_audio(self, path):\n",
    "        \"\"\"Load audio file and resample if needed\"\"\"\n",
    "        # Use soundfile directly to avoid torchaudio backend issues\n",
    "        waveform_np, sr = sf.read(path)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        waveform = torch.from_numpy(waveform_np).float()\n",
    "        \n",
    "        # Add channel dimension if mono\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        elif waveform.dim() == 2:\n",
    "            # If stereo, transpose to (channels, samples)\n",
    "            waveform = waveform.T\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        return waveform.squeeze(0)\n",
    "    \n",
    "    def pad_or_truncate(self, waveform):\n",
    "        \"\"\"Pad or truncate waveform to fixed length\"\"\"\n",
    "        if len(waveform) > self.max_samples:\n",
    "            # Truncate\n",
    "            waveform = waveform[:self.max_samples]\n",
    "        elif len(waveform) < self.max_samples:\n",
    "            # Pad\n",
    "            pad_length = self.max_samples - len(waveform)\n",
    "            waveform = F.pad(waveform, (0, pad_length))\n",
    "        \n",
    "        return waveform\n",
    "    \n",
    "    def normalize(self, waveform):\n",
    "        \"\"\"Normalize waveform\"\"\"\n",
    "        return waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "    \n",
    "    def extract_mel_spectrogram(self, waveform):\n",
    "        \"\"\"Extract mel-spectrogram features\"\"\"\n",
    "        mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=config.N_FFT,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            n_mels=config.N_MELS\n",
    "        )\n",
    "        \n",
    "        mel_spec = mel_transform(waveform)\n",
    "        mel_spec_db = T.AmplitudeToDB()(mel_spec)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    \n",
    "    def extract_mfcc(self, waveform):\n",
    "        \"\"\"Extract MFCC features\"\"\"\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=config.N_MFCC,\n",
    "            melkwargs={\n",
    "                'n_fft': config.N_FFT,\n",
    "                'hop_length': config.HOP_LENGTH,\n",
    "                'n_mels': config.N_MELS\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        mfcc = mfcc_transform(waveform)\n",
    "        return mfcc\n",
    "\n",
    "class AudioAugmentation:\n",
    "    \"\"\"Audio augmentation techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def add_noise(self, waveform, noise_factor=0.005):\n",
    "        \"\"\"Add Gaussian noise\"\"\"\n",
    "        noise = torch.randn_like(waveform) * noise_factor\n",
    "        return waveform + noise\n",
    "    \n",
    "    def time_shift(self, waveform, shift_max=0.2):\n",
    "        \"\"\"Shift audio in time\"\"\"\n",
    "        shift = int(np.random.uniform(-shift_max, shift_max) * len(waveform))\n",
    "        return torch.roll(waveform, shift)\n",
    "    \n",
    "    def pitch_shift(self, waveform, n_steps=2):\n",
    "        \"\"\"Shift pitch (using librosa)\"\"\"\n",
    "        waveform_np = waveform.numpy()\n",
    "        shifted = librosa.effects.pitch_shift(\n",
    "            waveform_np, \n",
    "            sr=self.sample_rate, \n",
    "            n_steps=np.random.uniform(-n_steps, n_steps)\n",
    "        )\n",
    "        return torch.from_numpy(shifted).float()\n",
    "    \n",
    "    def time_stretch(self, waveform, rate_range=(0.8, 1.2)):\n",
    "        \"\"\"Time stretching\"\"\"\n",
    "        rate = np.random.uniform(*rate_range)\n",
    "        waveform_np = waveform.numpy()\n",
    "        stretched = librosa.effects.time_stretch(waveform_np, rate=rate)\n",
    "        \n",
    "        # Pad or truncate to original length\n",
    "        if len(stretched) > len(waveform):\n",
    "            stretched = stretched[:len(waveform)]\n",
    "        else:\n",
    "            stretched = np.pad(stretched, (0, len(waveform) - len(stretched)))\n",
    "        \n",
    "        return torch.from_numpy(stretched).float()\n",
    "    \n",
    "    def apply_augmentation(self, waveform, prob=0.5):\n",
    "        \"\"\"Apply random augmentation\"\"\"\n",
    "        if np.random.random() < prob:\n",
    "            aug_type = np.random.choice(['noise', 'time_shift', 'pitch_shift', 'time_stretch'])\n",
    "            \n",
    "            if aug_type == 'noise':\n",
    "                return self.add_noise(waveform)\n",
    "            elif aug_type == 'time_shift':\n",
    "                return self.time_shift(waveform)\n",
    "            elif aug_type == 'pitch_shift':\n",
    "                return self.pitch_shift(waveform)\n",
    "            elif aug_type == 'time_stretch':\n",
    "                return self.time_stretch(waveform)\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "preprocessor = AudioPreprocessor(config.SAMPLE_RATE, config.MAX_DURATION)\n",
    "augmenter = AudioAugmentation(config.SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6e035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class AudioEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for audio emotion recognition\n",
    "    \n",
    "    Expected data structure:\n",
    "    data_root/\n",
    "        train/\n",
    "            emotion_0/\n",
    "                audio1.wav\n",
    "                audio2.wav\n",
    "            emotion_1/\n",
    "                ...\n",
    "        val/\n",
    "            ...\n",
    "    \n",
    "    OR CSV format with columns: audio_path, emotion_label\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, split='train', config=None, \n",
    "                 csv_path=None, use_augmentation=True):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.split = split\n",
    "        self.config = config or Config()\n",
    "        self.use_augmentation = use_augmentation and (split == 'train')\n",
    "        \n",
    "        # Load data\n",
    "        if csv_path:\n",
    "            self.samples = self._load_from_csv(csv_path)\n",
    "        else:\n",
    "            self.samples = self._load_from_directory()\n",
    "        \n",
    "        print(f\"{split} dataset: {len(self.samples)} samples\")\n",
    "        self._print_distribution()\n",
    "    \n",
    "    def _load_from_csv(self, csv_path):\n",
    "        \"\"\"Load dataset from CSV\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            samples.append({\n",
    "                'path': row['audio_path'],\n",
    "                'label': int(row['emotion_label'])\n",
    "            })\n",
    "        return samples\n",
    "    \n",
    "    def _load_from_directory(self):\n",
    "        \"\"\"Load dataset from directory structure\"\"\"\n",
    "        samples = []\n",
    "        split_dir = self.data_root / self.split\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            raise ValueError(f\"Directory {split_dir} does not exist\")\n",
    "        \n",
    "        for emotion_dir in split_dir.iterdir():\n",
    "            if not emotion_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            label = int(emotion_dir.name.split('_')[-1])\n",
    "            \n",
    "            for audio_path in emotion_dir.glob('*.wav'):\n",
    "                samples.append({'path': str(audio_path), 'label': label})\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _print_distribution(self):\n",
    "        \"\"\"Print label distribution\"\"\"\n",
    "        labels = [s['label'] for s in self.samples]\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\nLabel distribution in {self.split}:\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            emotion_name = self.config.EMOTION_LABELS.get(label, f\"class_{label}\")\n",
    "            print(f\"  {emotion_name}: {count} ({100*count/len(labels):.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform = preprocessor.load_audio(sample['path'])\n",
    "        \n",
    "        # Normalize\n",
    "        waveform = preprocessor.normalize(waveform)\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.use_augmentation:\n",
    "            waveform = augmenter.apply_augmentation(waveform, config.AUGMENT_PROB)\n",
    "        \n",
    "        # Pad or truncate\n",
    "        waveform = preprocessor.pad_or_truncate(waveform)\n",
    "        \n",
    "        label = sample['label']\n",
    "        \n",
    "        return waveform, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78cb7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class Wav2Vec2EmotionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wav2Vec2-based emotion recognition model\n",
    "    \n",
    "    Architecture:\n",
    "    - Wav2Vec2 Encoder (frozen initially) -> Temporal Features\n",
    "    - Optional LSTM for temporal modeling\n",
    "    - Classification Head -> Emotion Classes\n",
    "    - Extract hidden states for embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", \n",
    "                 num_classes=4, use_lstm=True, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.use_lstm = use_lstm\n",
    "        \n",
    "        # Load pretrained Wav2Vec2\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        self.embedding_dim = self.wav2vec2.config.hidden_size  # 768 for base\n",
    "        \n",
    "        # Temporal modeling with LSTM\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=self.embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=config.LSTM_LAYERS,\n",
    "                batch_first=True,\n",
    "                bidirectional=config.LSTM_BIDIRECTIONAL,\n",
    "                dropout=0.3 if config.LSTM_LAYERS > 1 else 0\n",
    "            )\n",
    "            \n",
    "            lstm_output_dim = hidden_dim * (2 if config.LSTM_BIDIRECTIONAL else 1)\n",
    "        else:\n",
    "            self.lstm = None\n",
    "            lstm_output_dim = self.embedding_dim\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_output_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(lstm_output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Attention pooling (alternative to mean pooling)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveform, return_embedding=False, return_temporal=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            waveform: Input tensor (B, seq_len)\n",
    "            return_embedding: Return utterance-level embedding\n",
    "            return_temporal: Return temporal sequence of embeddings\n",
    "        \n",
    "        Returns:\n",
    "            logits or embeddings based on flags\n",
    "        \"\"\"\n",
    "        # Extract features from Wav2Vec2\n",
    "        outputs = self.wav2vec2(waveform)\n",
    "        hidden_states = outputs.last_hidden_state  # (B, T, embedding_dim)\n",
    "        \n",
    "        # Temporal modeling with LSTM\n",
    "        if self.lstm:\n",
    "            lstm_out, _ = self.lstm(hidden_states)  # (B, T, lstm_output_dim)\n",
    "            features = lstm_out\n",
    "        else:\n",
    "            features = hidden_states\n",
    "        \n",
    "        # Return temporal embeddings if requested\n",
    "        if return_temporal:\n",
    "            return features\n",
    "        \n",
    "        # Attention-based pooling\n",
    "        attention_weights = self.attention(features)  # (B, T, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        utterance_embedding = torch.sum(features * attention_weights, dim=1)  # (B, lstm_output_dim)\n",
    "        \n",
    "        # Return embedding if requested\n",
    "        if return_embedding:\n",
    "            return utterance_embedding\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(utterance_embedding)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        \"\"\"Freeze Wav2Vec2 encoder\"\"\"\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Wav2Vec2 encoder frozen\")\n",
    "    \n",
    "    def unfreeze_encoder(self):\n",
    "        \"\"\"Unfreeze Wav2Vec2 encoder\"\"\"\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Wav2Vec2 encoder unfrozen\")\n",
    "    \n",
    "    def get_embedding(self, waveform):\n",
    "        \"\"\"Extract utterance-level embedding\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.forward(waveform, return_embedding=True)\n",
    "    \n",
    "    def get_temporal_embeddings(self, waveform):\n",
    "        \"\"\"Extract temporal sequence of embeddings\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.forward(waveform, return_temporal=True)\n",
    "\n",
    "class CNNSpectrogramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based emotion recognition from mel-spectrograms\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv layers on mel-spectrogram\n",
    "    - Temporal pooling\n",
    "    - LSTM for temporal modeling\n",
    "    - Classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4, n_mels=128, use_lstm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.use_lstm = use_lstm\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # Pool frequency dimension\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=512,\n",
    "                hidden_size=256,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                bidirectional=True,\n",
    "                dropout=0.3\n",
    "            )\n",
    "            lstm_output_dim = 512  # 256 * 2 (bidirectional)\n",
    "        else:\n",
    "            self.lstm = None\n",
    "            lstm_output_dim = 512\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, mel_spec, return_embedding=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            mel_spec: Mel-spectrogram (B, 1, n_mels, time)\n",
    "            return_embedding: Return embedding instead of logits\n",
    "        \"\"\"\n",
    "        # CNN feature extraction\n",
    "        features = self.conv_layers(mel_spec)  # (B, 512, 1, T)\n",
    "        features = features.squeeze(2).permute(0, 2, 1)  # (B, T, 512)\n",
    "        \n",
    "        # LSTM temporal modeling\n",
    "        if self.lstm:\n",
    "            lstm_out, _ = self.lstm(features)  # (B, T, 512)\n",
    "            # Mean pooling over time\n",
    "            embedding = torch.mean(lstm_out, dim=1)  # (B, 512)\n",
    "        else:\n",
    "            embedding = torch.mean(features, dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(embedding)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0fe5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.001, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self._is_improvement(score):\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "    \n",
    "    def _is_improvement(self, score):\n",
    "        if self.mode == 'min':\n",
    "            return score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            return score > (self.best_score + self.min_delta)\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Linear warmup scheduler\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fcec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "class AudioEmotionTrainer:\n",
    "    \"\"\"Training and validation logic for audio emotion recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = FocalLoss(alpha=None, gamma=2.0)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        num_training_steps = len(train_loader) * config.NUM_EPOCHS\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.WARMUP_STEPS,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if config.MIXED_PRECISION else None\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=config.PATIENCE,\n",
    "            min_delta=config.MIN_DELTA,\n",
    "            mode='max'  # Maximize accuracy\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.global_step = 0\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.NUM_EPOCHS}')\n",
    "        \n",
    "        for batch_idx, (waveforms, labels) in enumerate(pbar):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(waveforms)\n",
    "                    loss = self.criterion(outputs, labels) / self.config.ACCUMULATION_STEPS\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config.ACCUMULATION_STEPS == 0:\n",
    "                    if self.config.GRAD_CLIP:\n",
    "                        self.scaler.unscale_(self.optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n",
    "                    \n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "            else:\n",
    "                # Non-mixed precision training\n",
    "                outputs = self.model(waveforms)\n",
    "                loss = self.criterion(outputs, labels) / self.config.ACCUMULATION_STEPS\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config.ACCUMULATION_STEPS == 0:\n",
    "                    if self.config.GRAD_CLIP:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "            \n",
    "            # Calculate metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            acc = (preds == labels).float().mean()\n",
    "            \n",
    "            losses.update(loss.item() * self.config.ACCUMULATION_STEPS, waveforms.size(0))\n",
    "            accs.update(acc.item(), waveforms.size(0))\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': losses.avg, 'acc': accs.avg})\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return losses.avg, accs.avg, f1\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validation loop\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for waveforms, labels in tqdm(self.val_loader, desc='Validation'):\n",
    "                waveforms = waveforms.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = self.model(waveforms)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                acc = (preds == labels).float().mean()\n",
    "                \n",
    "                losses.update(loss.item(), waveforms.size(0))\n",
    "                accs.update(acc.item(), waveforms.size(0))\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return losses.avg, accs.avg, f1, all_preds, all_labels\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING AUDIO EMOTION TRAINING\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Freeze encoder initially\n",
    "        if self.config.FREEZE_ENCODER_EPOCHS > 0 and hasattr(self.model, 'freeze_encoder'):\n",
    "            self.model.freeze_encoder()\n",
    "        \n",
    "        for epoch in range(self.config.NUM_EPOCHS):\n",
    "            # Unfreeze encoder after specified epochs\n",
    "            if epoch == self.config.FREEZE_ENCODER_EPOCHS and hasattr(self.model, 'unfreeze_encoder'):\n",
    "                self.model.unfreeze_encoder()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc, train_f1 = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_f1, val_preds, val_labels = self.validate()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            self.history['lr'].append(self.scheduler.get_last_lr()[0])\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.NUM_EPOCHS}\")\n",
    "            print(f\"Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"✓ New best model! Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, is_best=False)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_acc):\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"Best Validation Accuracy: {self.best_val_acc:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'history': self.history,\n",
    "            'config': vars(self.config)\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            path = os.path.join(self.config.CHECKPOINT_DIR, 'best_audio_model.pth')\n",
    "        else:\n",
    "            path = os.path.join(self.config.CHECKPOINT_DIR, f'audio_checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        torch.save(checkpoint, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7feff657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. EMBEDDING EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class AudioEmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings from trained audio model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, config, model_type='wav2vec2'):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # Load model\n",
    "        if model_type == 'wav2vec2':\n",
    "            self.model = Wav2Vec2EmotionModel(\n",
    "                model_name=config.PRETRAINED_MODEL,\n",
    "                num_classes=config.NUM_CLASSES,\n",
    "                use_lstm=config.USE_LSTM,\n",
    "                hidden_dim=config.HIDDEN_DIM\n",
    "            ).to(self.device)\n",
    "        elif model_type == 'cnn_spectrogram':\n",
    "            self.model = CNNSpectrogramModel(\n",
    "                num_classes=config.NUM_CLASSES,\n",
    "                n_mels=config.N_MELS,\n",
    "                use_lstm=config.USE_LSTM\n",
    "            ).to(self.device)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Audio model loaded from {model_path}\")\n",
    "    \n",
    "    def extract_utterance_embedding(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract utterance-level embedding from audio file\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "        \n",
    "        Returns:\n",
    "            embedding: numpy array (embedding_dim,)\n",
    "        \"\"\"\n",
    "        # Load and preprocess audio\n",
    "        waveform = preprocessor.load_audio(audio_path)\n",
    "        waveform = preprocessor.normalize(waveform)\n",
    "        waveform = preprocessor.pad_or_truncate(waveform)\n",
    "        waveform = waveform.unsqueeze(0).to(self.device)  # Add batch dim\n",
    "        \n",
    "        # Extract embedding\n",
    "        with torch.no_grad():\n",
    "            if self.model_type == 'cnn_spectrogram':\n",
    "                mel_spec = preprocessor.extract_mel_spectrogram(waveform.squeeze(0))\n",
    "                mel_spec = mel_spec.unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "                embedding = self.model(mel_spec, return_embedding=True)\n",
    "            else:\n",
    "                embedding = self.model.get_embedding(waveform)\n",
    "        \n",
    "        return embedding.cpu().numpy().squeeze()\n",
    "    \n",
    "    def extract_temporal_embeddings(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract temporal sequence of embeddings (for multi-modal alignment)\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: numpy array (num_timesteps, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Load and preprocess\n",
    "        waveform = preprocessor.load_audio(audio_path)\n",
    "        waveform = preprocessor.normalize(waveform)\n",
    "        waveform = preprocessor.pad_or_truncate(waveform)\n",
    "        waveform = waveform.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Extract temporal embeddings\n",
    "        with torch.no_grad():\n",
    "            if hasattr(self.model, 'get_temporal_embeddings'):\n",
    "                embeddings = self.model.get_temporal_embeddings(waveform)\n",
    "            else:\n",
    "                # Fallback: get utterance-level embedding\n",
    "                embeddings = self.model.get_embedding(waveform).unsqueeze(1)\n",
    "        \n",
    "        return embeddings.cpu().numpy().squeeze()\n",
    "    \n",
    "    def extract_batch_embeddings(self, audio_paths):\n",
    "        \"\"\"Extract embeddings from batch of audio files\"\"\"\n",
    "        embeddings = []\n",
    "        for path in tqdm(audio_paths, desc='Extracting embeddings'):\n",
    "            emb = self.extract_utterance_embedding(path)\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d53184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. EVALUATION AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train', marker='o')\n",
    "    axes[1].plot(history['val_acc'], label='Val', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[2].plot(history['train_f1'], label='Train F1', marker='o')\n",
    "    axes[2].plot(history['val_f1'], label='Val F1', marker='s')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('F1 Score')\n",
    "    axes[2].set_title('F1 Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, save_path=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, config):\n",
    "    \"\"\"Comprehensive evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            waveforms = waveforms.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds,\n",
    "                                target_names=list(config.EMOTION_LABELS.values())))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_preds,\n",
    "                         list(config.EMOTION_LABELS.values()),\n",
    "                         save_path=os.path.join(config.OUTPUT_DIR, 'confusion_matrix_audio.png'))\n",
    "    \n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c206da1",
   "metadata": {},
   "source": [
    "# RAVDESS Dataset Preparation\n",
    "Process the downloaded RAVDESS dataset and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b059b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing RAVDESS dataset...\n",
      "================================================================================\n",
      "RAVDESS Dataset Preparation\n",
      "============================================================\n",
      "\n",
      "Emotion Mapping (4-class):\n",
      "  RAVDESS 01 -> 0: neutral\n",
      "  RAVDESS 03 -> 1: happy\n",
      "  RAVDESS 04 -> 2: sad\n",
      "  RAVDESS 05 -> 3: angry\n",
      "\n",
      "✓ Found Audio_Speech_Actors_01-24 subdirectory\n",
      "\n",
      "✓ Found 24 Actor directories\n",
      "\n",
      "✓ Found 672 audio files\n",
      "\n",
      "Emotion Distribution:\n",
      "  neutral: 96 samples (14.3%)\n",
      "  happy: 192 samples (28.6%)\n",
      "  sad: 192 samples (28.6%)\n",
      "  angry: 192 samples (28.6%)\n",
      "\n",
      "Dataset Splits (Speaker-Independent):\n",
      "  Train: 448 samples (16 actors)\n",
      "  Val:   84 samples (3 actors)\n",
      "  Test:  140 samples (5 actors)\n",
      "\n",
      "✓ Saved processed CSVs to: /Users/guptatilak/Desktop/multi-modal-mood-matcher/audio/data/processed\n",
      "\n",
      "Files created:\n",
      "  - data/processed/train.csv\n",
      "  - data/processed/val.csv\n",
      "  - data/processed/test.csv\n",
      "\n",
      "================================================================================\n",
      "✓ RAVDESS DATASET READY FOR TRAINING!\n",
      "================================================================================\n",
      "\n",
      "You can now run the main training cell below.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RAVDESS DATASET PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_ravdess_dataset(ravdess_dir=\"./data/ravdess\"):\n",
    "    \"\"\"\n",
    "    Prepare RAVDESS dataset for training\n",
    "    \n",
    "    RAVDESS filename format: 03-01-06-01-02-01-12.wav\n",
    "    - Modality (01 = full-AV, 02 = video-only, 03 = audio-only)\n",
    "    - Vocal channel (01 = speech, 02 = song)\n",
    "    - Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
    "    - Emotional intensity (01 = normal, 02 = strong)\n",
    "    - Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\")\n",
    "    - Repetition (01 = 1st repetition, 02 = 2nd repetition)\n",
    "    - Actor (01 to 24, odd numbered actors are male, even numbered actors are female)\n",
    "    \"\"\"\n",
    "    \n",
    "    ravdess_path = Path(ravdess_dir)\n",
    "    \n",
    "    if not ravdess_path.exists():\n",
    "        print(f\"❌ RAVDESS directory not found: {ravdess_path.absolute()}\")\n",
    "        print(\"\\nPlease ensure RAVDESS dataset is extracted to:\")\n",
    "        print(f\"  {ravdess_path.absolute()}\")\n",
    "        print(\"\\nExpected structure:\")\n",
    "        print(\"  data/ravdess/\")\n",
    "        print(\"    Audio_Speech_Actors_01-24/\")\n",
    "        print(\"      Actor_01/\")\n",
    "        print(\"        03-01-01-01-01-01-01.wav\")\n",
    "        print(\"        ...\")\n",
    "        print(\"      Actor_02/\")\n",
    "        print(\"        ...\")\n",
    "        return None\n",
    "    \n",
    "    print(\"RAVDESS Dataset Preparation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Emotion mapping (RAVDESS code -> our label)\n",
    "    # We'll use 4-class: neutral, happy, sad, angry\n",
    "    emotion_map = config.RAVDESS_EMOTION_MAP\n",
    "    \n",
    "    print(\"\\nEmotion Mapping (4-class):\")\n",
    "    for ravdess_code, label in emotion_map.items():\n",
    "        emotion_name = config.EMOTION_LABELS[label]\n",
    "        print(f\"  RAVDESS {ravdess_code} -> {label}: {emotion_name}\")\n",
    "    \n",
    "    # Check for Audio_Speech_Actors_01-24 subdirectory\n",
    "    audio_speech_dir = ravdess_path / \"Audio_Speech_Actors_01-24\"\n",
    "    if audio_speech_dir.exists():\n",
    "        print(f\"\\n✓ Found Audio_Speech_Actors_01-24 subdirectory\")\n",
    "        search_path = audio_speech_dir\n",
    "    else:\n",
    "        print(f\"\\nSearching in: {ravdess_path}\")\n",
    "        search_path = ravdess_path\n",
    "    \n",
    "    # Collect all audio files\n",
    "    samples = []\n",
    "    \n",
    "    actor_dirs = list(search_path.glob(\"Actor_*\"))\n",
    "    if len(actor_dirs) == 0:\n",
    "        print(f\"❌ No Actor_* directories found in {search_path}\")\n",
    "        print(f\"\\nAvailable directories:\")\n",
    "        for item in search_path.iterdir():\n",
    "            if item.is_dir():\n",
    "                print(f\"  - {item.name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(actor_dirs)} Actor directories\")\n",
    "    \n",
    "    for actor_dir in sorted(actor_dirs):\n",
    "        if not actor_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        audio_files = list(actor_dir.glob(\"*.wav\"))\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            # Parse filename\n",
    "            filename = audio_file.stem\n",
    "            parts = filename.split('-')\n",
    "            \n",
    "            if len(parts) != 7:\n",
    "                continue\n",
    "            \n",
    "            modality = parts[0]\n",
    "            vocal_channel = parts[1]\n",
    "            emotion_code = parts[2]\n",
    "            intensity = parts[3]\n",
    "            statement = parts[4]\n",
    "            repetition = parts[5]\n",
    "            actor = parts[6]\n",
    "            \n",
    "            # Filter: only audio-only (03) and speech (01)\n",
    "            if modality != '03' or vocal_channel != '01':\n",
    "                continue\n",
    "            \n",
    "            # Filter: only 4-class emotions\n",
    "            if emotion_code not in emotion_map:\n",
    "                continue\n",
    "            \n",
    "            samples.append({\n",
    "                'path': str(audio_file.absolute()),\n",
    "                'emotion_code': emotion_code,\n",
    "                'emotion_label': emotion_map[emotion_code],\n",
    "                'intensity': intensity,\n",
    "                'actor': actor,\n",
    "                'gender': 'M' if int(actor) % 2 == 1 else 'F'\n",
    "            })\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        print(f\"❌ No valid audio files found in {search_path}\")\n",
    "        print(\"\\nPlease check:\")\n",
    "        print(\"  1. Audio files exist in Actor_* directories\")\n",
    "        print(\"  2. Files follow RAVDESS naming convention (03-01-XX-XX-XX-XX-XX.wav)\")\n",
    "        print(\"  3. Files are audio-only (03) and speech (01)\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(samples)} audio files\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(samples)\n",
    "    \n",
    "    # Print emotion distribution\n",
    "    print(\"\\nEmotion Distribution:\")\n",
    "    for label in sorted(df['emotion_label'].unique()):\n",
    "        count = len(df[df['emotion_label'] == label])\n",
    "        emotion_name = config.EMOTION_LABELS[label]\n",
    "        print(f\"  {emotion_name}: {count} samples ({100*count/len(df):.1f}%)\")\n",
    "    \n",
    "    # Split into train/val/test (70/15/15)\n",
    "    # Stratify by both emotion and actor to ensure speaker independence\n",
    "    \n",
    "    # First, split by actor for speaker independence\n",
    "    actors = df['actor'].unique()\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(actors)\n",
    "    \n",
    "    n_actors = len(actors)\n",
    "    n_train = int(0.7 * n_actors)\n",
    "    n_val = int(0.15 * n_actors)\n",
    "    \n",
    "    train_actors = actors[:n_train]\n",
    "    val_actors = actors[n_train:n_train+n_val]\n",
    "    test_actors = actors[n_train+n_val:]\n",
    "    \n",
    "    train_df = df[df['actor'].isin(train_actors)].copy()\n",
    "    val_df = df[df['actor'].isin(val_actors)].copy()\n",
    "    test_df = df[df['actor'].isin(test_actors)].copy()\n",
    "    \n",
    "    print(f\"\\nDataset Splits (Speaker-Independent):\")\n",
    "    print(f\"  Train: {len(train_df)} samples ({len(train_actors)} actors)\")\n",
    "    print(f\"  Val:   {len(val_df)} samples ({len(val_actors)} actors)\")\n",
    "    print(f\"  Test:  {len(test_df)} samples ({len(test_actors)} actors)\")\n",
    "    \n",
    "    # Save split CSVs\n",
    "    output_dir = Path(\"./data/processed\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save with required columns for dataset loader\n",
    "    for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "        split_df_save = split_df[['path', 'emotion_label']].copy()\n",
    "        split_df_save.columns = ['audio_path', 'emotion_label']\n",
    "        split_df_save.to_csv(output_dir / f'{split_name}.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved processed CSVs to: {output_dir.absolute()}\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(f\"  - {output_dir / 'train.csv'}\")\n",
    "    print(f\"  - {output_dir / 'val.csv'}\")\n",
    "    print(f\"  - {output_dir / 'test.csv'}\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_df,\n",
    "        'val': val_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "\n",
    "# Prepare RAVDESS dataset\n",
    "print(\"Preparing RAVDESS dataset...\")\n",
    "print(\"=\" * 80)\n",
    "dataset_splits = prepare_ravdess_dataset()\n",
    "\n",
    "if dataset_splits:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ RAVDESS DATASET READY FOR TRAINING!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nYou can now run the main training cell below.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"⚠️ DATASET PREPARATION FAILED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nPlease ensure RAVDESS dataset is properly extracted to:\")\n",
    "    print(\"  ./data/ravdess/Audio_Speech_Actors_01-24/Actor_01/\")\n",
    "    print(\"  ./data/ravdess/Audio_Speech_Actors_01-24/Actor_02/\")\n",
    "    print(\"  ...\")\n",
    "    print(\"  ./data/ravdess/Audio_Speech_Actors_01-24/Actor_24/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90087cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUDIO EMOTION EMBEDDING MODEL TRAINING - RAVDESS\n",
      "================================================================================\n",
      "\n",
      "✓ Loading RAVDESS datasets from processed CSV files...\n",
      "train dataset: 448 samples\n",
      "\n",
      "Label distribution in train:\n",
      "  neutral: 64 (14.3%)\n",
      "  happy: 128 (28.6%)\n",
      "  sad: 128 (28.6%)\n",
      "  angry: 128 (28.6%)\n",
      "val dataset: 84 samples\n",
      "\n",
      "Label distribution in val:\n",
      "  neutral: 12 (14.3%)\n",
      "  happy: 24 (28.6%)\n",
      "  sad: 24 (28.6%)\n",
      "  angry: 24 (28.6%)\n",
      "\n",
      "Initializing wav2vec2 model...\n",
      "Total parameters: 106,319,237\n",
      "Trainable parameters: 106,319,237\n",
      "\n",
      "================================================================================\n",
      "STARTING AUDIO EMOTION TRAINING\n",
      "================================================================================\n",
      "\n",
      "Wav2Vec2 encoder frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 28/28 [19:11<00:00, 41.12s/it, loss=0.873, acc=0.192]\n",
      "Validation: 100%|██████████| 6/6 [00:49<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "Train - Loss: 0.8732 | Acc: 0.1920 | F1: 0.1622\n",
      "Val   - Loss: 0.7875 | Acc: 0.3452 | F1: 0.2710\n",
      "✓ New best model! Val Acc: 0.3452\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 28/28 [19:05<00:00, 40.90s/it, loss=0.838, acc=0.241]\n",
      "Validation: 100%|██████████| 6/6 [00:49<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/30\n",
      "Train - Loss: 0.8380 | Acc: 0.2411 | F1: 0.2151\n",
      "Val   - Loss: 0.7627 | Acc: 0.2976 | F1: 0.2118\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 28/28 [20:37<00:00, 44.18s/it, loss=0.82, acc=0.248] \n",
      "Validation: 100%|██████████| 6/6 [01:05<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/30\n",
      "Train - Loss: 0.8198 | Acc: 0.2478 | F1: 0.2381\n",
      "Val   - Loss: 0.7465 | Acc: 0.2976 | F1: 0.2222\n",
      "--------------------------------------------------------------------------------\n",
      "Wav2Vec2 encoder unfrozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 28/28 [29:22<00:00, 62.93s/it, loss=0.764, acc=0.286]\n",
      "Validation: 100%|██████████| 6/6 [00:44<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/30\n",
      "Train - Loss: 0.7637 | Acc: 0.2857 | F1: 0.2636\n",
      "Val   - Loss: 0.7279 | Acc: 0.3571 | F1: 0.2901\n",
      "✓ New best model! Val Acc: 0.3571\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 28/28 [29:53<00:00, 64.04s/it, loss=0.73, acc=0.364] \n",
      "Validation: 100%|██████████| 6/6 [00:44<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/30\n",
      "Train - Loss: 0.7297 | Acc: 0.3638 | F1: 0.3453\n",
      "Val   - Loss: 0.6931 | Acc: 0.3214 | F1: 0.2284\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 28/28 [25:25<00:00, 54.46s/it, loss=0.692, acc=0.411]\n",
      "Validation: 100%|██████████| 6/6 [00:47<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/30\n",
      "Train - Loss: 0.6915 | Acc: 0.4107 | F1: 0.3919\n",
      "Val   - Loss: 0.6388 | Acc: 0.4524 | F1: 0.4044\n",
      "✓ New best model! Val Acc: 0.4524\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 28/28 [24:21<00:00, 52.18s/it, loss=0.606, acc=0.429]\n",
      "Validation: 100%|██████████| 6/6 [00:45<00:00,  7.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/30\n",
      "Train - Loss: 0.6060 | Acc: 0.4286 | F1: 0.4117\n",
      "Val   - Loss: 0.6273 | Acc: 0.4643 | F1: 0.4127\n",
      "✓ New best model! Val Acc: 0.4643\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 28/28 [24:45<00:00, 53.05s/it, loss=0.548, acc=0.48] \n",
      "Validation: 100%|██████████| 6/6 [00:48<00:00,  8.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/30\n",
      "Train - Loss: 0.5483 | Acc: 0.4799 | F1: 0.4549\n",
      "Val   - Loss: 0.5257 | Acc: 0.5000 | F1: 0.4450\n",
      "✓ New best model! Val Acc: 0.5000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 28/28 [24:09<00:00, 51.77s/it, loss=0.503, acc=0.52] \n",
      "Validation: 100%|██████████| 6/6 [00:45<00:00,  7.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/30\n",
      "Train - Loss: 0.5029 | Acc: 0.5201 | F1: 0.5036\n",
      "Val   - Loss: 0.4479 | Acc: 0.5238 | F1: 0.4756\n",
      "✓ New best model! Val Acc: 0.5238\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 28/28 [24:31<00:00, 52.55s/it, loss=0.482, acc=0.542]\n",
      "Validation: 100%|██████████| 6/6 [00:48<00:00,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/30\n",
      "Train - Loss: 0.4818 | Acc: 0.5424 | F1: 0.5242\n",
      "Val   - Loss: 0.6036 | Acc: 0.4286 | F1: 0.3611\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 28/28 [24:37<00:00, 52.77s/it, loss=0.416, acc=0.616]\n",
      "Validation: 100%|██████████| 6/6 [00:45<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/30\n",
      "Train - Loss: 0.4158 | Acc: 0.6161 | F1: 0.6027\n",
      "Val   - Loss: 0.4999 | Acc: 0.4881 | F1: 0.4411\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 28/28 [24:47<00:00, 53.11s/it, loss=0.391, acc=0.562]\n",
      "Validation: 100%|██████████| 6/6 [00:59<00:00,  9.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/30\n",
      "Train - Loss: 0.3913 | Acc: 0.5625 | F1: 0.5572\n",
      "Val   - Loss: 0.4241 | Acc: 0.5952 | F1: 0.5334\n",
      "✓ New best model! Val Acc: 0.5952\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 28/28 [24:59<00:00, 53.56s/it, loss=0.321, acc=0.674]\n",
      "Validation: 100%|██████████| 6/6 [00:49<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/30\n",
      "Train - Loss: 0.3208 | Acc: 0.6741 | F1: 0.6724\n",
      "Val   - Loss: 0.3794 | Acc: 0.6071 | F1: 0.5260\n",
      "✓ New best model! Val Acc: 0.6071\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30:  61%|██████    | 17/28 [15:56<10:18, 56.26s/it, loss=0.308, acc=0.68] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApproximate frames per second: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemporal_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mconfig\u001b[38;5;241m.\u001b[39mMAX_DURATION\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 78\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     77\u001b[0m trainer \u001b[38;5;241m=\u001b[39m AudioEmotionTrainer(model, train_loader, val_loader, config)\n\u001b[0;32m---> 78\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Plot history\u001b[39;00m\n\u001b[1;32m     81\u001b[0m plot_training_history(\n\u001b[1;32m     82\u001b[0m     history,\n\u001b[1;32m     83\u001b[0m     save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mOUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_training_history.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 167\u001b[0m, in \u001b[0;36mAudioEmotionTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39munfreeze_encoder()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    170\u001b[0m val_loss, val_acc, val_f1, val_preds, val_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m, in \u001b[0;36mAudioEmotionTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 71\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mACCUMULATION_STEPS\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 76\u001b[0m, in \u001b[0;36mWav2Vec2EmotionModel.forward\u001b[0;34m(self, waveform, return_embedding, return_temporal)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mForward pass\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    logits or embeddings based on flags\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Extract features from Wav2Vec2\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# (B, T, embedding_dim)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Temporal modeling with LSTM\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1462\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1457\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1458\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1459\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m   1460\u001b[0m )\n\u001b[0;32m-> 1462\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:730\u001b[0m, in \u001b[0;36mWav2Vec2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    727\u001b[0m skip_the_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayerdrop\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_the_layer \u001b[38;5;129;01mor\u001b[39;00m synced_gpus:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# under fsdp or deepspeed zero3 all gpus must run in sync\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:496\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, early_stop, *args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m         )\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    499\u001b[0m         function, preserve, context_fn, determinism_check, debug, early_stop, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    500\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/autograd/function.py:581\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    259\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:618\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    617\u001b[0m     attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 618\u001b[0m     hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    622\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:557\u001b[0m, in \u001b[0;36mWav2Vec2Attention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, layer_head_mask, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    555\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 557\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    571\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[0;32m~/Desktop/multi-modal-mood-matcher/audio/venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_not(attention_mask\u001b[38;5;241m.\u001b[39mbool())\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 96\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline for RAVDESS dataset\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"AUDIO EMOTION EMBEDDING MODEL TRAINING - RAVDESS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load processed CSV files\n",
    "    processed_dir = Path(\"./data/processed\")\n",
    "    train_csv = str(processed_dir / 'train.csv')\n",
    "    val_csv = str(processed_dir / 'val.csv')\n",
    "    \n",
    "    print(\"\\n✓ Loading RAVDESS datasets from processed CSV files...\")\n",
    "    \n",
    "    # Create datasets from CSV\n",
    "    train_dataset = AudioEmotionDataset(\n",
    "        data_root=\"./data\",\n",
    "        split='train',\n",
    "        config=config,\n",
    "        csv_path=train_csv,\n",
    "        use_augmentation=config.USE_AUGMENTATION\n",
    "    )\n",
    "    \n",
    "    val_dataset = AudioEmotionDataset(\n",
    "        data_root=\"./data\",\n",
    "        split='val',\n",
    "        config=config,\n",
    "        csv_path=val_csv,\n",
    "        use_augmentation=False\n",
    "    )\n",
    "    \n",
    "    # Data loaders - USE num_workers=0 to avoid multiprocessing issues\n",
    "    # (librosa operations in augmentation don't work well with multiprocessing)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Changed from 4 to 0 to avoid worker process issues\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Changed from 4 to 0 to avoid worker process issues\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing {config.MODEL_TYPE} model...\")\n",
    "    if config.MODEL_TYPE == 'wav2vec2':\n",
    "        model = Wav2Vec2EmotionModel(\n",
    "            model_name=config.PRETRAINED_MODEL,\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            use_lstm=config.USE_LSTM,\n",
    "            hidden_dim=config.HIDDEN_DIM\n",
    "        )\n",
    "    elif config.MODEL_TYPE == 'cnn_spectrogram':\n",
    "        model = CNNSpectrogramModel(\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            n_mels=config.N_MELS,\n",
    "            use_lstm=config.USE_LSTM\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Train\n",
    "    trainer = AudioEmotionTrainer(model, train_loader, val_loader, config)\n",
    "    history = trainer.train()\n",
    "    \n",
    "    # Plot history\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=os.path.join(config.OUTPUT_DIR, 'audio_training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal evaluation...\")\n",
    "    model_path = os.path.join(config.CHECKPOINT_DIR, 'best_audio_model.pth')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    val_preds, val_labels = evaluate_model(model, val_loader, config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Best model: {model_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Demonstration\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMONSTRATION: EMBEDDING EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    extractor = AudioEmbeddingExtractor(model_path, config, config.MODEL_TYPE)\n",
    "    \n",
    "    # Example\n",
    "    sample_waveform, sample_label = val_dataset[0]\n",
    "    sample_audio_path = val_dataset.samples[0]['path']\n",
    "    \n",
    "    # Utterance-level embedding\n",
    "    utterance_emb = extractor.extract_utterance_embedding(sample_audio_path)\n",
    "    print(f\"\\nUtterance embedding shape: {utterance_emb.shape}\")\n",
    "    print(f\"True emotion: {config.EMOTION_LABELS[sample_label]}\")\n",
    "    \n",
    "    # Temporal embeddings\n",
    "    if config.MODEL_TYPE == 'wav2vec2':\n",
    "        temporal_emb = extractor.extract_temporal_embeddings(sample_audio_path)\n",
    "        print(f\"Temporal embeddings shape: {temporal_emb.shape}\")\n",
    "        print(f\"Approximate frames per second: {temporal_emb.shape[0] / config.MAX_DURATION:.1f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
