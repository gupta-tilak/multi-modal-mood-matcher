{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5062aeba",
   "metadata": {},
   "source": [
    "Multi-Modal Mood Matcher - Visual Feature Extraction Training Notebook\n",
    "========================================================================\n",
    "This notebook trains a ResNet-50/EfficientNet backbone for emotion recognition\n",
    "and extracts identity-invariant, emotion-sensitive embeddings.\n",
    "\n",
    "Target: Train on AffectNet/RAF-DB, extract penultimate layer embeddings (512-D or 2048-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedf6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guptatilak/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import timm  # For EfficientNet and other modern architectures\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Face Detection (MediaPipe alternative using OpenCV)\n",
    "# For production, integrate MediaPipe: pip install mediapipe\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    MEDIAPIPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MEDIAPIPE_AVAILABLE = False\n",
    "    print(\"MediaPipe not available. Using OpenCV cascade for face detection.\")\n",
    "\n",
    "# Metrics and Logging\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb  # Optional: for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e8f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079af672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a42e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = \"./data/affectnet\"  # Change to your dataset path\n",
    "    OUTPUT_DIR = \"./outputs/emotion_model\"\n",
    "    CHECKPOINT_DIR = \"./checkpoints\"\n",
    "    \n",
    "    # Model Architecture\n",
    "    BACKBONE = \"efficientnet_b3\"  # Options: resnet50, efficientnet_b0, efficientnet_b3\n",
    "    PRETRAINED = True\n",
    "    EMBEDDING_DIM = 1536  # ResNet50: 2048, EfficientNet-B0: 1280, EfficientNet-B3: 1536\n",
    "    NUM_CLASSES = 8  # AffectNet: neutral, happy, sad, surprise, fear, disgust, anger, contempt\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    SCHEDULER = \"cosine\"  # Options: cosine, step, plateau\n",
    "    \n",
    "    # Data Augmentation\n",
    "    IMG_SIZE = 224\n",
    "    TRAIN_AUGMENT = True\n",
    "    \n",
    "    # Training Strategy\n",
    "    FREEZE_BACKBONE_EPOCHS = 5  # Freeze backbone for first N epochs\n",
    "    GRAD_CLIP = 1.0\n",
    "    MIXED_PRECISION = True  # Use automatic mixed precision\n",
    "    \n",
    "    # Early Stopping\n",
    "    PATIENCE = 10\n",
    "    MIN_DELTA = 0.001\n",
    "    \n",
    "    # Logging\n",
    "    USE_WANDB = False  # Set to True to enable W&B logging\n",
    "    WANDB_PROJECT = \"emotion-embedding\"\n",
    "    \n",
    "    # Emotion Labels (AffectNet) - mapping from string labels to indices\n",
    "    EMOTION_TO_IDX = {\n",
    "        'neutral': 0,\n",
    "        'happy': 1,\n",
    "        'sad': 2,\n",
    "        'surprise': 3,\n",
    "        'fear': 4,\n",
    "        'disgust': 5,\n",
    "        'anger': 6,\n",
    "        'contempt': 7\n",
    "    }\n",
    "    \n",
    "    # Reverse mapping for display\n",
    "    EMOTION_LABELS = {v: k for k, v in EMOTION_TO_IDX.items()}\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df23f2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766729195.353764       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. FACE DETECTION AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class FaceDetector:\n",
    "    \"\"\"Face detection and alignment using MediaPipe or OpenCV\"\"\"\n",
    "    \n",
    "    def __init__(self, use_mediapipe=MEDIAPIPE_AVAILABLE):\n",
    "        self.use_mediapipe = use_mediapipe\n",
    "        \n",
    "        if self.use_mediapipe:\n",
    "            self.mp_face_detection = mp.solutions.face_detection\n",
    "            self.face_detection = self.mp_face_detection.FaceDetection(\n",
    "                model_selection=1, \n",
    "                min_detection_confidence=0.5\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to OpenCV Haar Cascade\n",
    "            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "            self.face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    def detect_and_crop(self, image):\n",
    "        \"\"\"\n",
    "        Detect face and return cropped, aligned face image\n",
    "        Args:\n",
    "            image: numpy array (H, W, 3) in RGB\n",
    "        Returns:\n",
    "            cropped_face: numpy array or None if no face detected\n",
    "        \"\"\"\n",
    "        if self.use_mediapipe:\n",
    "            results = self.face_detection.process(image)\n",
    "            if not results.detections:\n",
    "                return None\n",
    "            \n",
    "            detection = results.detections[0]  # Take first face\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = image.shape\n",
    "            \n",
    "            x = int(bboxC.xmin * w)\n",
    "            y = int(bboxC.ymin * h)\n",
    "            w_box = int(bboxC.width * w)\n",
    "            h_box = int(bboxC.height * h)\n",
    "            \n",
    "            # Add margin\n",
    "            margin = 0.2\n",
    "            x = max(0, int(x - w_box * margin))\n",
    "            y = max(0, int(y - h_box * margin))\n",
    "            w_box = int(w_box * (1 + 2 * margin))\n",
    "            h_box = int(h_box * (1 + 2 * margin))\n",
    "            \n",
    "            cropped = image[y:y+h_box, x:x+w_box]\n",
    "            return cropped if cropped.size > 0 else None\n",
    "        else:\n",
    "            # OpenCV detection\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "            \n",
    "            if len(faces) == 0:\n",
    "                return None\n",
    "            \n",
    "            # Get image dimensions\n",
    "            img_h, img_w = image.shape[:2]\n",
    "            \n",
    "            x, y, w, h = faces[0]  # Take first face\n",
    "            margin = int(0.2 * w)\n",
    "            x = max(0, x - margin)\n",
    "            y = max(0, y - margin)\n",
    "            w = min(w + 2 * margin, img_w - x)  # Ensure we don't exceed image width\n",
    "            h = min(h + 2 * margin, img_h - y)  # Ensure we don't exceed image height\n",
    "            \n",
    "            cropped = image[y:y+h, x:x+w]\n",
    "            return cropped if cropped.size > 0 else None\n",
    "\n",
    "face_detector = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bda8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for emotion recognition from face images\n",
    "    \n",
    "    Expected data structure:\n",
    "    data_root/\n",
    "        Train/\n",
    "            anger/\n",
    "            happy/\n",
    "            ...\n",
    "        Test/\n",
    "            Anger/\n",
    "            happy/\n",
    "            ...\n",
    "        labels.csv (optional - with columns: pth, label)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, split='Train', transform=None, \n",
    "                 csv_path=None, use_face_detection=True):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.use_face_detection = use_face_detection\n",
    "        \n",
    "        # Load data\n",
    "        if csv_path:\n",
    "            self.samples = self._load_from_csv(csv_path)\n",
    "        else:\n",
    "            self.samples = self._load_from_directory()\n",
    "        \n",
    "        print(f\"{split} dataset: {len(self.samples)} samples\")\n",
    "        self._print_distribution()\n",
    "    \n",
    "    def _load_from_csv(self, csv_path):\n",
    "        \"\"\"Load dataset from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            # CSV has columns: pth, label (string)\n",
    "            img_path = self.data_root / self.split / row['pth']\n",
    "            if img_path.exists():\n",
    "                label_str = row['label'].lower()  # Normalize to lowercase\n",
    "                if label_str in config.EMOTION_TO_IDX:\n",
    "                    samples.append({\n",
    "                        'path': str(img_path),\n",
    "                        'label': config.EMOTION_TO_IDX[label_str]\n",
    "                    })\n",
    "        return samples\n",
    "    \n",
    "    def _load_from_directory(self):\n",
    "        \"\"\"Load dataset from directory structure\"\"\"\n",
    "        samples = []\n",
    "        split_dir = self.data_root / self.split\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            raise ValueError(f\"Split directory not found: {split_dir}\")\n",
    "        \n",
    "        for emotion_dir in split_dir.iterdir():\n",
    "            if not emotion_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Get emotion label from folder name (normalize to lowercase)\n",
    "            emotion_name = emotion_dir.name.lower()\n",
    "            \n",
    "            if emotion_name not in config.EMOTION_TO_IDX:\n",
    "                print(f\"Warning: Unknown emotion folder '{emotion_dir.name}', skipping...\")\n",
    "                continue\n",
    "            \n",
    "            label = config.EMOTION_TO_IDX[emotion_name]\n",
    "            \n",
    "            for img_path in emotion_dir.glob('*'):\n",
    "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    samples.append({'path': str(img_path), 'label': label})\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _print_distribution(self):\n",
    "        \"\"\"Print label distribution\"\"\"\n",
    "        labels = [s['label'] for s in self.samples]\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\nLabel distribution in {self.split}:\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            emotion_name = config.EMOTION_LABELS.get(label, f\"class_{label}\")\n",
    "            print(f\"  {emotion_name}: {count} ({100*count/len(labels):.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(sample['path'])\n",
    "        if image is None:\n",
    "            # Return a black image if loading fails\n",
    "            image = np.zeros((config.IMG_SIZE, config.IMG_SIZE, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Face detection and cropping\n",
    "        if self.use_face_detection:\n",
    "            face = face_detector.detect_and_crop(image)\n",
    "            if face is None:\n",
    "                # If no face detected, use whole image\n",
    "                face = image\n",
    "        else:\n",
    "            face = image\n",
    "        \n",
    "        # Convert to PIL for transforms\n",
    "        face = Image.fromarray(face)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            face = self.transform(face)\n",
    "        \n",
    "        label = sample['label']\n",
    "        \n",
    "        return face, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d537712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def get_transforms(split='train', img_size=224):\n",
    "    \"\"\"Get data transforms for training and validation\"\"\"\n",
    "    \n",
    "    if split == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c901ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class EmotionEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Emotion recognition model with embedding extraction capability\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone (ResNet50/EfficientNet) -> Feature Extractor\n",
    "    - Penultimate Layer -> Embedding (512/2048-D)\n",
    "    - Classification Head -> Emotion Classes\n",
    "    \n",
    "    For inference: Remove classification head and extract embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='resnet50', num_classes=7, \n",
    "                 pretrained=True, embedding_dim=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Load backbone\n",
    "        if backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove final FC layer\n",
    "            \n",
    "        elif backbone.startswith('efficientnet'):\n",
    "            self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "            in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        \n",
    "        # Embedding projection (if needed)\n",
    "        if in_features != embedding_dim:\n",
    "            self.embedding_projection = nn.Sequential(\n",
    "                nn.Linear(in_features, embedding_dim),\n",
    "                nn.BatchNorm1d(embedding_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding_projection = nn.Identity()\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for new layers\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (B, 3, H, W)\n",
    "            return_embedding: If True, return embedding instead of logits\n",
    "        \n",
    "        Returns:\n",
    "            If return_embedding=True: embedding (B, embedding_dim)\n",
    "            Else: logits (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Extract features from backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding_projection(features)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embeddings\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone weights\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Backbone frozen\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone weights\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone unfrozen\")\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Extract embedding (for inference)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x, return_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96feb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.001, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self._is_improvement(score):\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def _is_improvement(self, score):\n",
    "        if self.mode == 'min':\n",
    "            return score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            return score > (self.best_score + self.min_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1952d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training and validation logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Loss function (use Focal Loss for imbalanced datasets)\n",
    "        self.criterion = FocalLoss(alpha=None, gamma=2.0)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if config.SCHEDULER == 'cosine':\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer, T_max=config.NUM_EPOCHS\n",
    "            )\n",
    "        elif config.SCHEDULER == 'step':\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(\n",
    "                self.optimizer, step_size=10, gamma=0.1\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', patience=5, factor=0.5\n",
    "            )\n",
    "        \n",
    "        # Mixed precision training - only enable on CUDA\n",
    "        if config.MIXED_PRECISION and torch.cuda.is_available():\n",
    "            self.scaler = torch.amp.GradScaler('cuda')\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=config.PATIENCE, \n",
    "            min_delta=config.MIN_DELTA,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.NUM_EPOCHS}')\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if self.config.GRAD_CLIP:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), self.config.GRAD_CLIP\n",
    "                    )\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                if self.config.GRAD_CLIP:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), self.config.GRAD_CLIP\n",
    "                    )\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            acc = (preds == labels).float().mean()\n",
    "            \n",
    "            # Update meters\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            accs.update(acc.item(), images.size(0))\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{losses.avg:.4f}',\n",
    "                'acc': f'{accs.avg:.4f}',\n",
    "                'lr': f'{self.optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "        \n",
    "        return losses.avg, accs.avg\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc='Validating'):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                acc = (preds == labels).float().mean()\n",
    "                \n",
    "                losses.update(loss.item(), images.size(0))\n",
    "                accs.update(acc.item(), images.size(0))\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        return losses.avg, accs.avg, all_preds, all_labels\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING TRAINING\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Freeze backbone for initial epochs if specified\n",
    "        if self.config.FREEZE_BACKBONE_EPOCHS > 0:\n",
    "            self.model.freeze_backbone()\n",
    "        \n",
    "        for epoch in range(self.config.NUM_EPOCHS):\n",
    "            # Unfreeze backbone after specified epochs\n",
    "            if epoch == self.config.FREEZE_BACKBONE_EPOCHS:\n",
    "                self.model.unfreeze_backbone()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_preds, val_labels = self.validate()\n",
    "            \n",
    "            # Update scheduler\n",
    "            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                self.scheduler.step(val_loss)\n",
    "            else:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['lr'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.NUM_EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"✓ New best model saved! Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, is_best=False)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss):\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"Best Validation Accuracy: {self.best_val_acc:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'history': self.history,\n",
    "            'config': vars(self.config)\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            path = os.path.join(self.config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "        else:\n",
    "            path = os.path.join(self.config.CHECKPOINT_DIR, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3131326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. EVALUATION AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[2].plot(history['lr'], label='Learning Rate', marker='o', color='orange')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, save_path=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Get embeddings\n",
    "            embeddings = model(images, return_embedding=True)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, \n",
    "                                target_names=list(config.EMOTION_LABELS.values())))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_preds, \n",
    "                         list(config.EMOTION_LABELS.values()),\n",
    "                         save_path=os.path.join(config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
    "    \n",
    "    return all_embeddings, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec20043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10. EMBEDDING EXTRACTION FOR INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "class EmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings from trained model for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, config):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Load model\n",
    "        self.model = EmotionEmbeddingModel(\n",
    "            backbone=config.BACKBONE,\n",
    "            num_classes=config.NUM_CLASSES,\n",
    "            embedding_dim=config.EMBEDDING_DIM\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        print(f\"Embedding dimension: {config.EMBEDDING_DIM}\")\n",
    "    \n",
    "    def extract_embedding(self, image):\n",
    "        \"\"\"\n",
    "        Extract embedding from a single image\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array (H, W, 3)\n",
    "        \n",
    "        Returns:\n",
    "            embedding: numpy array of shape (embedding_dim,)\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        transform = get_transforms(split='val', img_size=self.config.IMG_SIZE)\n",
    "        \n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        image_tensor = transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Extract embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model.get_embedding(image_tensor)\n",
    "        \n",
    "        return embedding.cpu().numpy().squeeze()\n",
    "    \n",
    "    def extract_embeddings_batch(self, images):\n",
    "        \"\"\"\n",
    "        Extract embeddings from a batch of images\n",
    "        \n",
    "        Args:\n",
    "            images: List of PIL Images or numpy arrays\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: numpy array of shape (N, embedding_dim)\n",
    "        \"\"\"\n",
    "        transform = get_transforms(split='val', img_size=self.config.IMG_SIZE)\n",
    "        \n",
    "        image_tensors = []\n",
    "        for img in images:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img = Image.fromarray(img)\n",
    "            image_tensors.append(transform(img))\n",
    "        \n",
    "        batch = torch.stack(image_tensors).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.get_embedding(batch)\n",
    "        \n",
    "        return embeddings.cpu().numpy()\n",
    "    \n",
    "    def extract_video_embeddings(self, video_path, sample_rate=5):\n",
    "        \"\"\"\n",
    "        Extract embeddings from video frames\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to video file\n",
    "            sample_rate: Extract every Nth frame\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: numpy array of shape (num_frames, embedding_dim)\n",
    "            frame_indices: List of frame indices\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        embeddings = []\n",
    "        frame_indices = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % sample_rate == 0:\n",
    "                # Convert BGR to RGB\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Detect and crop face\n",
    "                face = face_detector.detect_and_crop(frame_rgb)\n",
    "                if face is not None:\n",
    "                    # Extract embedding\n",
    "                    embedding = self.extract_embedding(face)\n",
    "                    embeddings.append(embedding)\n",
    "                    frame_indices.append(frame_count)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        return np.array(embeddings), frame_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717ae1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EMOTION EMBEDDING MODEL TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Loading datasets...\n",
      "Train dataset: 16108 samples\n",
      "\n",
      "Label distribution in Train:\n",
      "  neutral: 2758 (17.1%)\n",
      "  happy: 2340 (14.5%)\n",
      "  sad: 3091 (19.2%)\n",
      "  surprise: 2119 (13.2%)\n",
      "  fear: 1512 (9.4%)\n",
      "  disgust: 1229 (7.6%)\n",
      "  anger: 1500 (9.3%)\n",
      "  contempt: 1559 (9.7%)\n",
      "Test dataset: 14518 samples\n",
      "\n",
      "Label distribution in Test:\n",
      "  neutral: 2368 (16.3%)\n",
      "  happy: 2704 (18.6%)\n",
      "  sad: 1584 (10.9%)\n",
      "  surprise: 1920 (13.2%)\n",
      "  fear: 1664 (11.5%)\n",
      "  disgust: 1248 (8.6%)\n",
      "  anger: 1718 (11.8%)\n",
      "  contempt: 1312 (9.0%)\n",
      "\n",
      "Initializing model: efficientnet_b3\n",
      "Total parameters: 11,488,304\n",
      "Trainable parameters: 11,488,304\n",
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n",
      "Backbone frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 252/252 [26:43<00:00,  6.36s/it, loss=11.2302, acc=0.1874, lr=0.000100]\n",
      "Validating: 100%|██████████| 227/227 [23:47<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Train Loss: 11.2302 | Train Acc: 0.1874\n",
      "Val Loss: 5.1088 | Val Acc: 0.2823\n",
      "LR: 0.000100\n",
      "✓ New best model saved! Val Acc: 0.2823\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 252/252 [27:50<00:00,  6.63s/it, loss=8.1157, acc=0.2722, lr=0.000100]\n",
      "Validating: 100%|██████████| 227/227 [25:05<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50\n",
      "Train Loss: 8.1157 | Train Acc: 0.2722\n",
      "Val Loss: 4.4158 | Val Acc: 0.3302\n",
      "LR: 0.000100\n",
      "✓ New best model saved! Val Acc: 0.3302\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 252/252 [36:29<00:00,  8.69s/it, loss=7.2560, acc=0.2998, lr=0.000100]\n",
      "Validating: 100%|██████████| 227/227 [27:17<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50\n",
      "Train Loss: 7.2560 | Train Acc: 0.2998\n",
      "Val Loss: 4.0638 | Val Acc: 0.3456\n",
      "LR: 0.000099\n",
      "✓ New best model saved! Val Acc: 0.3456\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50:  64%|██████▍   | 162/252 [21:40<12:02,  8.03s/it, loss=6.9971, acc=0.3130, lr=0.000099]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding (first 10 values): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     66\u001b[39m trainer = Trainer(model, train_loader, val_loader, config)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m     72\u001b[39m plot_training_history(\n\u001b[32m     73\u001b[39m     history,\n\u001b[32m     74\u001b[39m     save_path=os.path.join(config.OUTPUT_DIR, \u001b[33m'\u001b[39m\u001b[33mtraining_history.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.unfreeze_backbone()\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m train_loss, train_acc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    170\u001b[39m val_loss, val_acc, val_preds, val_labels = \u001b[38;5;28mself\u001b[39m.validate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, labels)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mEmotionEmbeddingModel.forward\u001b[39m\u001b[34m(self, x, return_embedding)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03mForward pass\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m    Else: logits (B, num_classes)\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Extract features from backbone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Get embeddings\u001b[39;00m\n\u001b[32m     88\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.embedding_projection(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/timm/models/efficientnet.py:347\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    346\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/timm/models/efficientnet.py:325\u001b[39m, in \u001b[36mEfficientNet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    323\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x, flatten=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv_head(x)\n\u001b[32m    327\u001b[39m x = \u001b[38;5;28mself\u001b[39m.bn2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/timm/models/_efficientnet_blocks.py:329\u001b[39m, in \u001b[36mInvertedResidual.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    327\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv_s2d(x)\n\u001b[32m    328\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn_s2d(x)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_pw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    331\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv_dw(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multi-modal-mood-matcher/visual/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 11. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EMOTION EMBEDDING MODEL TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize W&B (optional)\n",
    "    if config.USE_WANDB:\n",
    "        wandb.init(project=config.WANDB_PROJECT, config=vars(config))\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_dataset = EmotionDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        split='Train',  # Use actual folder name\n",
    "        transform=get_transforms('train', config.IMG_SIZE),\n",
    "        use_face_detection=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = EmotionDataset(\n",
    "        data_root=config.DATA_ROOT,\n",
    "        split='Test',  # Use actual folder name (Test instead of val)\n",
    "        transform=get_transforms('val', config.IMG_SIZE),\n",
    "        use_face_detection=True\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    # Note: num_workers=0 required for Jupyter notebooks (multiprocessing issues)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for Jupyter notebook compatibility\n",
    "        pin_memory=False  # Disable for MPS/CPU\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Set to 0 for Jupyter notebook compatibility\n",
    "        pin_memory=False  # Disable for MPS/CPU\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing model: {config.BACKBONE}\")\n",
    "    model = EmotionEmbeddingModel(\n",
    "        backbone=config.BACKBONE,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        pretrained=config.PRETRAINED,\n",
    "        embedding_dim=config.EMBEDDING_DIM\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(model, train_loader, val_loader, config)\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train()\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=os.path.join(config.OUTPUT_DIR, 'training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nFinal evaluation on validation set...\")\n",
    "    model_path = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    val_embeddings, val_labels, val_preds = evaluate_model(model, val_loader)\n",
    "    \n",
    "    # Save embeddings for analysis\n",
    "    np.save(os.path.join(config.OUTPUT_DIR, 'val_embeddings.npy'), val_embeddings)\n",
    "    np.save(os.path.join(config.OUTPUT_DIR, 'val_labels.npy'), val_labels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Best model saved at: {model_path}\")\n",
    "    print(f\"Outputs saved in: {config.OUTPUT_DIR}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Demonstration of embedding extraction\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMONSTRATION: EMBEDDING EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    extractor = EmbeddingExtractor(model_path, config)\n",
    "    \n",
    "    # Example: Extract embedding from first validation image\n",
    "    sample_image, sample_label = val_dataset[0]\n",
    "    sample_image_pil = transforms.ToPILImage()(sample_image)\n",
    "    \n",
    "    embedding = extractor.extract_embedding(sample_image_pil)\n",
    "    print(f\"\\nExtracted embedding shape: {embedding.shape}\")\n",
    "    print(f\"True emotion: {config.EMOTION_LABELS[sample_label]}\")\n",
    "    print(f\"Embedding (first 10 values): {embedding[:10]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
